{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionCLIPModel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vaOjIJHmdlccuoCNTYmBeePtQv-4FxqV",
      "authorship_tag": "ABX9TyOsiPxTSf9NANSzWNtDq3CI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmunagal/FashionCLIPModel/blob/main/FashionCLIPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HihNb_iBUPY4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import pickle \n",
        "import os \n",
        "from PIL import Image\n",
        "\n",
        "import glob\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPImage\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/pickels/16k_apperal_data_preprocessed\",\"rb\") as input_file:\n",
        "    \n",
        "    output_file =pickle.load(input_file)"
      ],
      "metadata": {
        "id": "SYd2Tj5QR-B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file['product_type_name'].unique()"
      ],
      "metadata": {
        "id": "De8zexNRV188"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file.columns"
      ],
      "metadata": {
        "id": "Z4m9xYj6WC9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file.head(5)"
      ],
      "metadata": {
        "id": "4EhqqEmnWxq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = output_file.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "dKwAOX9GYUtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_fn(image):\n",
        "    return image+'.jpeg'"
      ],
      "metadata": {
        "id": "EaUhxYgnYj2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file['Image_name']=output_file.asin.apply(image_fn)"
      ],
      "metadata": {
        "id": "34eyg63jYW8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir images "
      ],
      "metadata": {
        "id": "JrMC4pbhut7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/drive/MyDrive/16k_images  images "
      ],
      "metadata": {
        "id": "sqHrnO01uv2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(IPImage(('/content/images/16k_images/0000000060.jpeg'), width=200))"
      ],
      "metadata": {
        "id": "7R3q6b0wZriD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install timm\n",
        "! pip install sentence_transformers"
      ],
      "metadata": {
        "id": "gFwxtK8G7VQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch \n",
        "import numpy as np \n",
        "import albumentations as A\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW \n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.models as models\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.nn.functional import normalize\n",
        "from torch.nn.functional import softmax\n",
        "from sentence_transformers import util"
      ],
      "metadata": {
        "id": "zSP-0ESob5RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , AutoModel\n",
        "\n",
        "class config:\n",
        "    dimension = 256 # both image & text will be resized to 256 \n",
        "    text_model='distilbert-base-uncased'\n",
        "    image_model = 'resnet34'\n",
        "    image_path = '/content/images/16k_images'\n",
        "    text_path = '/content/drive/MyDrive/pickels/'\n",
        "    tokenizer=AutoTokenizer.from_pretrained(text_model)\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 16 \n",
        "    epochs=10\n",
        "    image_size = 250\n",
        "    sentence_max_lenght=30\n",
        "    image_output_dim = 256 \n",
        "    text_output_dim=256\n",
        "    output_dim_text = 768\n",
        "    outout_dim_image=512\n",
        "\n"
      ],
      "metadata": {
        "id": "kg73qLlJVqYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDataset:\n",
        "    def __init__(self,image,text):\n",
        "\n",
        "        self.image=image \n",
        "        self.text=text \n",
        "\n",
        "        self.image_aug= A.Compose(\n",
        "            [\n",
        "                A.Resize(config.image_size,config.image_size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.text)  \n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "         \n",
        "      \n",
        "        text=self.text[idx] \n",
        "\n",
        "        text=config.tokenizer(text,max_length=config.sentence_max_lenght,truncation=True,padding='max_length')\n",
        "\n",
        "\n",
        "       \n",
        "        image = cv2.imread(f\"{config.image_path}/{self.image[idx]}\") # reading the image \n",
        "    \n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to grey scale image \n",
        "\n",
        "        image = self.image_aug(image=image)['image']\n",
        "    \n",
        "    \n",
        "        return {\n",
        "        \n",
        "         'image':torch.tensor(image).permute(2, 0, 1).float(),\n",
        "         'ids':torch.tensor(text['input_ids'],dtype=torch.long),\n",
        "         'mask':torch.tensor(text['attention_mask'],dtype=torch.long)\n",
        "\n",
        "\n",
        "         }\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "9QhZ_AuUVqU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionEncodeImageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(FashionEncodeImageModel,self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "        self.restnet50model = timm.create_model(config.image_model,num_classes=0,pretrained=True, global_pool=\"avg\")\n",
        "\n",
        "   \n",
        "    def forward(self,image_data):\n",
        "        output=self.restnet50model(image_data)  \n",
        "        return output  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LORFn2ZiVqNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionTextModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionTextModel,self).__init__()\n",
        "\n",
        "        self.bert_model = AutoModel.from_pretrained(config.text_model)\n",
        "   \n",
        "\n",
        "\n",
        "    def forward(self,input_ids,attention_mask):\n",
        "        output = self.bert_model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "\n",
        "        output=output.last_hidden_state[:,0,:]\n",
        "\n",
        "        return output "
      ],
      "metadata": {
        "id": "NbNBKHH3QUg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionClipModel(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # text model dim conversion \n",
        "\n",
        "        super(FashionClipModel,self).__init__()\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "        self.text_embedding=nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512,config.text_output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.LayerNorm(config.text_output_dim)\n",
        "          \n",
        "        )\n",
        "        # Image model dim conversion  \n",
        "\n",
        "        self.image_embeding=nn.Sequential(\n",
        "\n",
        "            nn.Linear(512,config.text_output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LayerNorm(config.image_output_dim)\n",
        "        )\n",
        "\n",
        "        self.text_model =FashionTextModel()\n",
        "        self.image_model = FashionEncodeImageModel() \n",
        "\n",
        "\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self,image,ids,attention_mask):\n",
        "\n",
        "        out_image = self.image_model(image) # reset model output \n",
        "\n",
        "        text_output = self.text_model(ids,attention_mask) # output from bert model \n",
        "\n",
        "\n",
        "        text_output= self.text_embedding(text_output)\n",
        "\n",
        "        out_image=self.image_embeding(out_image)\n",
        "\n",
        "\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale *  out_image @ text_output.T # cosine similarity calculation .\n",
        "        logits_per_text = logits_per_image.T\n",
        "\n",
        "\n",
        "        return logits_per_image , logits_per_text\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MZGfzOTNQUcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=FashionClipModel()"
      ],
      "metadata": {
        "id": "eQ2Rxtk6QUZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE=('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "frY0ColzVYmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(DEVICE)"
      ],
      "metadata": {
        "id": "Haf7FkQsVhea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'# of trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
        "print(f'# of non-trainable params: {sum(p.numel() for p in model.parameters() if not p.requires_grad):,}')"
      ],
      "metadata": {
        "id": "pfgLqdUtU45H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file.columns"
      ],
      "metadata": {
        "id": "vGHVR3-YvBLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_dataset= output_file[['title','Image_name']]"
      ],
      "metadata": {
        "id": "tu6u2FPZVrI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_dataset.drop(11195,axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "gslNEZ-FWCHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This images doesnt contain the data in the 16k images which we want to train \n",
        "image_name=[\n",
        "'B07147JSY5.jpeg',\n",
        "'B0714D3YBH.jpeg',\n",
        "'B0714DWP9R.jpeg',\n",
        "'B0714BMPJQ.jpeg',\n",
        "'B071454LGB.jpeg',\n",
        "'B0714DWVNG.jpeg',\n",
        "'B0714B1RG2.jpeg',\n",
        "'B07148TD5Z.jpeg',\n",
        "'B07145W3G3.jpeg',\n",
        "'B071469DM9.jpeg',\n",
        "'B07145F1VR.jpeg',\n",
        "'B0714C9M7S.jpeg',\n",
        "'B07148V8SN.jpeg',\n",
        "'B07148BVMF.jpeg',\n",
        "'B07144VH39.jpeg',\n",
        "'B0714DWR46.jpeg',\n",
        "'B07146SR2T.jpeg',\n",
        "'B07144GTFJ.jpeg',\n",
        "'B0714CXPTN.jpeg'\n",
        "]"
      ],
      "metadata": {
        "id": "Gv2dzdfG1Fx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in image_name:\n",
        "    fashion_dataset.drop(fashion_dataset[fashion_dataset['Image_name']==x].index,axis=0,inplace=True) \n"
      ],
      "metadata": {
        "id": "iSxugr-w1Y74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_dataset=fashion_dataset.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AsepS2X8xCVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_dataset[fashion_dataset['Image_name']=='B0714CXPTN.jpeg']"
      ],
      "metadata": {
        "id": "cLo9b_r36jQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(input , target):\n",
        "\n",
        "    loss_cal=nn.CrossEntropyLoss()\n",
        "    loss=loss_cal(input,target)\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "JEd8EpBSXERB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLossManual:\n",
        "    \"\"\"\n",
        "    y0 is the vector with shape (batch_size,C)\n",
        "    x shape is the same (batch_size), whose entries are integers from 0 to C-1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ignore_index=-100) -> None:\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def __call__(self, y0, x):\n",
        "        loss = 0.\n",
        "        n_batch, n_class = y0.shape\n",
        "        for y1, x1 in zip(y0, x):\n",
        "            class_index = int(x1.item())\n",
        "\n",
        "            if class_index == self.ignore_index:\n",
        "                n_batch -= 1\n",
        "                continue\n",
        "            loss = loss + torch.log(torch.exp(y1[class_index]) / (torch.exp(y1).sum()))\n",
        "        loss = - loss / n_batch\n",
        "        return loss\n",
        "\n",
        "\n",
        "loss=CrossEntropyLossManual()"
      ],
      "metadata": {
        "id": "RAnZnFL4ZAiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model,dataloader,optimizer,scheduler):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss=0\n",
        "\n",
        "    tq=tqdm(dataloader , total=len(dataloader))\n",
        "\n",
        "    for batch_size , data in enumerate(tq):\n",
        "\n",
        "\n",
        "        count = data['image'].size(0)\n",
        "\n",
        "    \n",
        "\n",
        "        image=data['image']\n",
        "        ids=data['ids']\n",
        "        attention_mask = data['mask']\n",
        "\n",
        "        image = image.to(DEVICE)\n",
        "\n",
        "        ids=ids.to(DEVICE)\n",
        "\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "         \n",
        "        logits_per_image, logits_per_text = model(image,ids,attention_mask)\n",
        "        \n",
        "\n",
        "        #  calculate the loss \n",
        "\n",
        "        target = torch.arange(count)\n",
        "\n",
        "        target=target.to(DEVICE)\n",
        "\n",
        "       \n",
        "\n",
        "        loss_image = loss_fn(logits_per_image,target)\n",
        "\n",
        "        loss_text =  loss_fn(logits_per_text, target.T)\n",
        "\n",
        "\n",
        "        total_loss = loss_image+loss_text/2.0\n",
        "\n",
        "  \n",
        "\n",
        "        train_loss+=total_loss.item()\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "\n",
        "        nn.utils.clip_grad_value_(model.parameters(), clip_value=2.0)\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "     \n",
        "    return train_loss/len(dataloader)\n"
      ],
      "metadata": {
        "id": "BdeWCJm9RRcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_fn(model,dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss=0\n",
        "\n",
        "    tqd=tqdm(dataloader , total=len(dataloader))\n",
        "\n",
        "    for batch_size , data in enumerate(tqd):\n",
        "\n",
        "        count = data['image'].size(0)\n",
        "\n",
        "        image=data['image']\n",
        "        ids=data['ids']\n",
        "        attention_mask = data['mask']\n",
        "\n",
        "        image = image.to(DEVICE)\n",
        "\n",
        "        ids=ids.to(DEVICE)\n",
        "\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "\n",
        "        logits_per_image, logits_per_text = model(image,ids,attention_mask)\n",
        "\n",
        "\n",
        "            # calculate the loss \n",
        "\n",
        "        target = torch.arange(count)\n",
        "\n",
        "        target=target.to(DEVICE)\n",
        "\n",
        "        texts_loss = loss_fn(logits_per_text , target)\n",
        "        images_loss = loss_fn(logits_per_image, target.T)\n",
        "        loss =  (images_loss + texts_loss) / 2.0 \n",
        "\n",
        "\n",
        "        eval_loss+=loss.item()\n",
        "        \n",
        "     \n",
        "    return eval_loss/len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_khJ46yKhy8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    x_train , x_test = train_test_split(fashion_dataset,test_size=0.33 , random_state=42)\n",
        "\n",
        "    x_train = x_train.reset_index(drop=True)\n",
        "\n",
        "    x_test = x_test.reset_index(drop=True)\n",
        "\n",
        "    train_dataset= FashionDataset(x_train.Image_name,x_train.title)\n",
        "\n",
        "    test_dataset = FashionDataset(x_test.Image_name,x_test.title)\n",
        "\n",
        "    train_data_loader=torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size\n",
        "    )\n",
        "    \n",
        "    valid_data_loader=torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.eval_batch_size\n",
        "    )\n",
        "\n",
        "\n",
        "    num_train_steps = len(train_dataset)/config.train_batch_size* config.epochs\n",
        "\n",
        "    # optimized_parameters \n",
        "\n",
        "    optimizer_parameters = list(model.parameters())\n",
        "\n",
        "\n",
        "    optimizer=AdamW(optimizer_parameters,lr=2e-5)\n",
        "\n",
        " \n",
        "    scheduler=ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.1,\n",
        "       patience=1\n",
        "    \n",
        "    )\n",
        "    \n",
        "    path='/content/drive/MyDrive/FashionModel/model1.pt'\n",
        "\n",
        "    \n",
        "    best_loss = float('inf')\n",
        "\n",
        " \n",
        "    for x in range(config.epochs):\n",
        "\n",
        "        print(f' Current epoch value was {x+1} out of {config.epochs}')\n",
        "\n",
        "        \n",
        "        loss_train = train_fn(model,train_data_loader,optimizer,scheduler)\n",
        "        \n",
        "        loss_val = eval_fn( model,valid_data_loader)\n",
        "\n",
        "     \n",
        "        \n",
        "        print(f' The epoch {x+1} encountered a  train loss of : {loss_train} and validation loss of {loss_val}')  \n",
        "\n",
        "        scheduler.step(loss_val)\n",
        "\n",
        "\n",
        "        if loss_val <  best_loss:\n",
        "\n",
        "            torch.save(model.state_dict(), path)\n",
        "\n",
        "            print(\"Saved Best Model!\")\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CpJtXu3-QUV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd images"
      ],
      "metadata": {
        "id": "ylNnU_U1Cdfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "id": "iaI3hJhSQDOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "y5G015793VVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LslJOSSv3XsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=FashionClipModel()"
      ],
      "metadata": {
        "id": "zthEeOg23nm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FashionModel/model.pt',map_location=DEVICE))\n",
        "# reloading the model \n",
        "\n",
        "model.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "bTem5jH93qTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train , x_test = train_test_split(fashion_dataset,test_size=0.33 , random_state=42)\n",
        "\n",
        "x_train = x_train.reset_index(drop=True)\n",
        "\n",
        "x_test = x_test.reset_index(drop=True)\n",
        "\n",
        "train_dataset= FashionDataset(x_train.Image_name,x_train.title)\n",
        "\n",
        "test_dataset = FashionDataset(x_test.Image_name,x_test.title)\n",
        "\n",
        "train_data_loader=torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size\n",
        "    )\n",
        "    \n",
        "valid_data_loader=torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.eval_batch_size\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KAkFLvfi4cru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "hA7zT1ftGz7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.iloc[18,:].values"
      ],
      "metadata": {
        "id": "s34tf7Lp4oTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd images"
      ],
      "metadata": {
        "id": "Awnze92pWeDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -la"
      ],
      "metadata": {
        "id": "YdIp6G8RWwL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(valid_data_loader):\n",
        " \n",
        "    \n",
        "    valid_image_embeddings = []\n",
        "    with torch.inference_mode(mode=True):\n",
        "        for data in tqdm(valid_data_loader):\n",
        "            image_features = model.image_model(data['image'].to(DEVICE))\n",
        "            image_embeddings = model.image_embeding(image_features)\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "    return torch.cat(valid_image_embeddings)"
      ],
      "metadata": {
        "id": "aT2B88Zi4KpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=get_image_embeddings(valid_data_loader)"
      ],
      "metadata": {
        "id": "2rNYQFk85Ypq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_names=list(x_test.iloc[:,1])"
      ],
      "metadata": {
        "id": "IGNj-TxB-avR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "Qt0kRh7KAZxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def find_match_text_image(model, image_embeddings, text,image_file):\n",
        "    text=config.tokenizer(text,max_length=config.sentence_max_lenght,truncation=True,padding='max_length')\n",
        "    \n",
        "    input_ids = torch.tensor(text['input_ids']).unsqueeze(0)\n",
        "\n",
        "    attention_mask = torch.tensor(text[\"attention_mask\"]).unsqueeze(0)\n",
        "    \n",
        "    with torch.inference_mode(mode=True):\n",
        "        text_features = model.text_model(\n",
        "            input_ids=input_ids.to(DEVICE) ,attention_mask=attention_mask.to(DEVICE)\n",
        "        )\n",
        "        text_embeddings = model.text_embedding(text_features)\n",
        "\n",
        "      \n",
        "\n",
        " \n",
        "    image_embeddings_n = normalize(image_embeddings, p=2)\n",
        "   \n",
        "    text_embedding = normalize(text_embeddings, p=2)\n",
        "\n",
        "    dot_similarity = text_embeddings @ image_embeddings.T * model.logit_scale\n",
        "\n",
        "    # output=util.semantic_search(text_embeddings , image_embeddings ,top_k=6)\n",
        "\n",
        "\n",
        "    \n",
        "    # values, indices = torch.topk(dot_similarity.squeeze(0), 9 * 5)\n",
        "\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), 5*6)\n",
        "    matches = [image_file[idx] for idx in indices[::5]]\n",
        "\n",
        "    # top_k_images=[]\n",
        "    # for x in output[0]:\n",
        "    #     top_k_images.append(image_file[x['corpus_id']])\n",
        "\n",
        "    \n",
        "    _, axes = plt.subplots(2, 3, figsize=(10, 10))\n",
        "    for match , ax  in zip(matches ,axes.flatten()):\n",
        "   \n",
        "        image = cv2.imread(f\"{config.image_path}/{match}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(image)\n",
        "        ax.axis(\"off\")\n",
        "       \n",
        "       \n"
      ],
      "metadata": {
        "id": "6W6whd3_5o-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(IPImage(('/content/images/16k_images/B06ZXWCW8L.jpeg'), width=200))"
      ],
      "metadata": {
        "id": "CnqVajG1DAmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data='bar iii sleeveless buttondown tunic deep black xs'\n",
        "find_match_text_image(model,output,data,images_names)"
      ],
      "metadata": {
        "id": "iame-ZFdGoy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}